{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport dask\nimport dask.dataframe as dd\nimport featuretools as ft\nimport h2o\nimport numpy as np\nimport json\nfrom pandas.io.json import json_normalize\nfrom sklearn.model_selection import KFold\nfrom functools import partial\nfrom itertools import count, repeat, cycle\n\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_train(dataset, dataset_event_data, cutting_sec=10000):\n    game_sequence = (dataset\n    .drop_duplicates(subset=['game_session'], keep='first')\n        [['event_id', 'game_session', 'timestamp', 'installation_id', 'type', 'title', 'world', 'game_time']]\n        .reset_index(drop=True)\n    )\n\n    game_sequence_y = (game_sequence\n        .query('type == \"Assessment\"')\n        .reset_index(drop=True)\n        .copy()\n    )\n\n    game_sequence_filter = (game_sequence\n        .merge(game_sequence_y, on='installation_id', how='inner', suffixes=('_x', '_y'))\n        .assign(diff = lambda df: (df['timestamp_y'] - df['timestamp_x']).dt.total_seconds())\n        .query('0 <= diff < {0}'.format(cutting_sec)) # determine the cutting gap\n        .reset_index(drop=True)\n        [['game_session_x', 'game_session_y']]\n    )\n    \n    dataset_df = add_event_data_info(dataset, dataset_event_data, game_sequence_filter)  \n    return dataset_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_test(dataset, dataset_event_data, cutting_sec=10000):\n    game_sequence_filter = (dataset\n        .assign(game_session_y = lambda df: df['installation_id'])\n        .merge(dataset.groupby('installation_id')['timestamp'].max().reset_index().rename(columns={'timestamp': 'timestamp_y'}), on='installation_id', how='inner')\n        .rename(columns={'game_session': 'game_session_x', 'timestamp': 'timestamp_x'})\n        .assign(diff = lambda df: (df['timestamp_y'] - df['timestamp_x']).dt.total_seconds())\n        .query('0 <= diff < {0}'.format(cutting_sec)) # determine the cutting gap\n        [['game_session_x', 'game_session_y']]\n        .drop_duplicates(subset=['game_session_x', 'game_session_y'], keep='first')\n    )\n    \n    dataset_df = add_event_data_info(dataset, dataset_event_data, game_sequence_filter)  \n    \n    return dataset_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_event_data_info(dataset, dataset_event_data, game_sequence_filter):\n    def flatten_(record, sel_cols):\n        return dict(map(lambda x: (x, record.get(x)), sel_cols))\n\n    event_data_cols = ['coordinates',\n                       'correct', 'duration', 'dwell_time', 'misses', 'round', 'total_duration', 'version']\n    event_data_cols_meta = {\n        'correct': np.bool, \n        'duration': np.float16, \n        'dwell_time': np.float32, \n        'misses': np.float32, \n        'round': np.float16, \n        'total_duration': np.float32, \n        'version': np.float16\n    }\n    flatten_ = partial(flatten_, sel_cols=event_data_cols)\n\n    event_data = dataset_event_data['event_data'].to_bag().map(flatten_).to_dataframe(meta=event_data_cols_meta)\n    dataset = (dd.concat([dataset, event_data], axis=1)\n        .merge(game_sequence_filter, left_on='game_session', right_on='game_session_x', how='inner')\n        .query('game_session_y != game_session') # filter out game_session_y data to prevent data leakage\n        .drop(columns=['game_session_x'])\n    )\n    \n    dataset_df = dataset.assign(timestamp = lambda df: df['timestamp'].dt.tz_localize(None)).compute(scheduler='threads').reset_index(drop=True)\n    \n    return dataset_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_entityset(dataset_df):\n    es = ft.EntitySet(id=\"game_session_y_data\")\n\n    es = es.entity_from_dataframe(entity_id=\"actions\",\n                                  dataframe=dataset_df,\n                                  index='action_id',\n                                  time_index='timestamp',\n                                  make_index=True)\n\n    es = es.normalize_entity(base_entity_id=\"actions\",\n                             new_entity_id=\"game_sessions\",\n                             index=\"game_session\",\n                             additional_variables=[\"title\", \"type\", \"world\", \"game_session_y\", \"installation_id\"])\n\n    es = es.normalize_entity(base_entity_id=\"game_sessions\",\n                             new_entity_id=\"game_session_ys\",\n                             index=\"game_session_y\",\n                             additional_variables=[\"installation_id\"])\n\n    es = es.normalize_entity(base_entity_id=\"game_session_ys\",\n                             new_entity_id=\"installation_ids\",\n                             index=\"installation_id\")\n    \n    return es","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_feature_matrix(dataset_es):\n    feature_matrix, feature_defs = ft.dfs(entityset=dataset_es, target_entity=\"game_session_ys\")    \n    return feature_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_feature_pipe(dataset, dataset_event_data):\n    return create_feature_matrix(create_entityset(preprocess_train(dataset, dataset_event_data)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_feature_pipe(dataset, dataset_event_data):\n    return create_feature_matrix(create_entityset(preprocess_test(dataset, dataset_event_data)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### valid label function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_installation_kfold_label(train_df, n_split):\n    installations = train_df['installation_id'].unique()\n    install_ser = pd.Series(installations)\n    c = cycle(range(n_split))\n    \n    result = (install_ser\n        .sample(frac=1)\n        .to_frame(name='installation_id')\n        .assign(fold_label = list(map(lambda _: next(c), range(len(install_ser)))))\n        .sort_index()\n    )\n    \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_session_order_label(train_df):\n    last_assessment_valid = (train_df\n        .groupby(['game_session_y', 'installation_id'], as_index=False)['timestamp']\n        .max()\n        .assign(session_order_label = lambda df: df.groupby('installation_id')['timestamp'].rank(ascending=False).astype(int))\n        [['game_session_y', 'session_order_label']]\n    )\n    \n    return last_assessment_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/khoongweihao/bayesian-opt-seed-blending-with-tuning-69\n\nimport scipy as sp\n\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n        return -cohen_kappa_score(y, X_p)\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(self.coef_['x'])) + [np.inf], labels = [0, 1, 2, 3])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_dtype = {\n    'event_id':'object', 'game_session':'object', 'installation_id':'object',\n    'event_count':'int16', 'event_code':'category', 'game_time':'int32', 'title':'category', \n    'type':'category', 'world':'category'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Dataset\ntrain = dd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv', \n                    parse_dates=['timestamp'], \n                    dtype=col_dtype,\n                    usecols=['event_id', 'game_session', 'timestamp', 'installation_id', 'event_count', 'event_code', 'game_time', 'title', 'type', 'world'])\n\ntrain_event_data = dd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv', \n                               converters={'event_data': json.loads},\n                               usecols=['event_data'])\n\n# Train target Column\ntrain_labels_df = dd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv').compute(scheduler='threads')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Dataset\ntest = dd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv', \n                    parse_dates=['timestamp'], \n                    dtype=col_dtype,\n                    usecols=['event_id', 'game_session', 'timestamp', 'installation_id', 'event_count', 'event_code', 'game_time', 'title', 'type', 'world'])\n\ntest_event_data = dd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv', \n                               converters={'event_data': json.loads},\n                               usecols=['event_data'])\n\nsample_submission = dd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv').compute(scheduler='threads')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Train Features\n# train_features = train_feature_pipe(train, train_event_data)\n# create_feature_matrix(create_entityset(preprocess_train(dataset, dataset_event_data)))\n\n# Raw Training Cutting Data\ntrain_df = preprocess_train(train, train_event_data, cutting_sec=10000)\n\n# Featuretools EntitySet\ntrain_es = create_entityset(train_df)\n\n# Featuretools Feature\ntrain_features = create_feature_matrix(train_es)\n\n\n# Validation Label\ninstallation_kfold_label = create_installation_kfold_label(train_df, n_split=5)\nsession_order_label = create_session_order_label(train_df)\n\n# Train Features add target column 'accuracy_group', installation_kfold_label, session_order_label\ntrain_data_df = (train_features\n    .reset_index()\n    .merge(installation_kfold_label, on='installation_id', how='inner') # add kfold by installation_id\n    .merge(session_order_label, on='game_session_y', how='inner') # add session order, session_order_label==1 is last assessment\n    .merge(train_labels_df[['game_session', 'accuracy_group']], left_on='game_session_y', right_on='game_session', how='inner') # add target label\n    .drop(columns=['game_session_y', 'game_session', 'installation_id'])\n    .reset_index(drop=True)\n)\n\n# Train Column Schema\ntarget_col = 'accuracy_group'\nfeature_cols = train_data_df.columns.drop([target_col]).to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Test Features\n#test_features = test_feature_pipe(test, test_event_data)\n\n# Raw testing Cutting Data\ntest_df = preprocess_test(test, test_event_data, cutting_sec=10000)\n\n# Featuretools EntitySet\ntest_es = create_entityset(test_df)\n\n# Featuretools Feature\ntest_features = create_feature_matrix(test_es)\n\n# \ntest_data_df = (test_features\n    .drop(columns=['installation_id'])\n    [feature_cols]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## h2o"},{"metadata":{"trusted":true},"cell_type":"code","source":"from h2o.automl import H2OAutoML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score, confusion_matrix\n\nh2o.init()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_hf = h2o.H2OFrame(train_data_df)\n\ntrain = train_data_hf[train_data_hf['session_order_label'] != 1]\nvalid = train_data_hf[train_data_hf['session_order_label'] == 1]\n\n# Identify predictors and response\nx = train.columns\ny = \"accuracy_group\"\nx.remove(y)\n\ntrain[y] = train[y]\nvalid[y] = valid[y]\naml = H2OAutoML(max_models=10, seed=1, stopping_metric='RMSE')\naml.train(x=x, y=y, training_frame=train, validation_frame=valid, fold_column='fold_label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opr = OptimizedRounder()\nopr.fit(aml.predict(train).as_data_frame()['predict'], train[y].as_data_frame().values.reshape(-1))\ny_preds = opr.predict(aml.predict(valid).as_data_frame()['predict'])\ny_trues = valid[y].as_data_frame()['accuracy_group']\ncohen_kappa_score(y_trues, y_preds, weights='quadratic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_trues, y_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multiclass"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data_hf = h2o.H2OFrame(train_data_df)\n\n# train, valid = train_data_hf.split_frame(ratios=[0.7], seed=1)\n\n# # Identify predictors and response\n# x = train.columns\n# y = \"accuracy_group\"\n# x.remove(y)\n\n# train[y] = train[y].asfactor()\n# valid[y] = valid[y].asfactor()\n\n# aml = H2OAutoML(max_runtime_secs=1200, seed=1, stopping_metric='RMSE')\n# aml.train(x=x, y=y, training_frame=train, validation_frame=valid)\n\n# # View the AutoML Leaderboard\n# lb = aml.leaderboard\n# lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)\n\n# y_preds = aml.predict(valid).as_data_frame()['predict']\n# y_trues = valid[y].as_data_frame()['accuracy_group']\n\n# cohen_kappa_score(y_trues, y_preds, weights='quadratic')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}