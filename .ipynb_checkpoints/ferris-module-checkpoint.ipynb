{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./input/train_labels.csv\n",
      "./input/test.csv\n",
      "./input/specs.csv\n",
      "./input/train.csv\n",
      "./input/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import featuretools as ft\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "import time\n",
    "import shap\n",
    "import sys, os, psutil\n",
    "import cmath\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pandas.io.json import json_normalize\n",
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "from itertools import count, repeat\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "    \n",
    "    \n",
    "def cpuStats():\n",
    "    print(\"########## CPU STATS ############\")\n",
    "    print(sys.version)\n",
    "    print(psutil.cpu_percent())\n",
    "    print(psutil.virtual_memory())  # physical memory usage\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0] / 2. ** 30\n",
    "    print('memory GB:', memoryUse)\n",
    "    print(\"########## CPU STATS ############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(dataset, dataset_event_data, cutting_sec=10000):\n",
    "    game_sequence = (dataset\n",
    "    .drop_duplicates(subset=['game_session'], keep='first')\n",
    "        [['event_id', 'game_session', 'timestamp', 'installation_id', 'type', 'title', 'world', 'game_time']]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    game_sequence_y = (game_sequence\n",
    "        .query('type == \"Assessment\"')\n",
    "        .reset_index(drop=True)\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    game_sequence_filter = (game_sequence\n",
    "        .merge(game_sequence_y, on='installation_id', how='inner', suffixes=('_x', '_y'))\n",
    "        .assign(diff = lambda df: (df['timestamp_y'] - df['timestamp_x']).dt.total_seconds())\n",
    "        .query('0 <= diff < {0}'.format(cutting_sec)) # determine the cutting gap\n",
    "        .reset_index(drop=True)\n",
    "        [['game_session_x', 'game_session_y']]\n",
    "    )\n",
    "    \n",
    "    dataset_df = add_event_data_info(dataset, dataset_event_data, game_sequence_filter)  \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(dataset, dataset_event_data, cutting_sec=10000):\n",
    "    game_sequence_filter = (dataset\n",
    "        .assign(game_session_y = lambda df: df['installation_id'])\n",
    "        .merge(dataset.groupby('installation_id')['timestamp'].max().reset_index().rename(columns={'timestamp': 'timestamp_y'}), on='installation_id', how='inner')\n",
    "        .rename(columns={'game_session': 'game_session_x', 'timestamp': 'timestamp_x'})\n",
    "        .assign(diff = lambda df: (df['timestamp_y'] - df['timestamp_x']).dt.total_seconds())\n",
    "        .query('0 <= diff < {0}'.format(cutting_sec)) # determine the cutting gap\n",
    "        [['game_session_x', 'game_session_y']]\n",
    "        .drop_duplicates(subset=['game_session_x', 'game_session_y'], keep='first')\n",
    "    )\n",
    "    \n",
    "    dataset_df = add_event_data_info(dataset, dataset_event_data, game_sequence_filter)  \n",
    "    \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_event_data_info(dataset, dataset_event_data, game_sequence_filter):\n",
    "    \n",
    "    def flatten_(record, sel_cols):\n",
    "        return dict(map(lambda x: (x, record.get(x)), sel_cols))\n",
    "\n",
    "    event_data_cols = ['coordinates',\n",
    "                       'correct', 'duration', 'dwell_time', \n",
    "                       'misses', 'round', 'total_duration', 'version']\n",
    "    event_data_cols_meta = {\n",
    "        'correct': np.bool, \n",
    "        'duration': np.float16, \n",
    "        'dwell_time': np.float32, \n",
    "        'misses': np.float32, \n",
    "        'round': np.float16, \n",
    "        'total_duration': np.float32, \n",
    "        'version': np.float16\n",
    "    }\n",
    "    flatten_ = partial(flatten_, sel_cols=event_data_cols)\n",
    "\n",
    "    event_data = dataset_event_data['event_data'].to_bag().map(flatten_).to_dataframe(meta=event_data_cols_meta)\n",
    "    dataset = (dd.concat([dataset, event_data], axis=1)\n",
    "        .merge(game_sequence_filter, left_on='game_session', right_on='game_session_x', how='inner')\n",
    "        .query('game_session_y != game_session') # filter out game_session_y data to prevent data leakage\n",
    "        .drop(columns=['game_session_x'])\n",
    "    )\n",
    "    \n",
    "    dataset_df = dataset.compute(scheduler='threads').reset_index(drop=True)\n",
    "    \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entityset(dataset_df):\n",
    "    es = ft.EntitySet(id=\"game_session_y_data\")\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id=\"actions\",\n",
    "                                  dataframe=dataset_df,\n",
    "                                  index='action_id',\n",
    "                                  make_index=True)\n",
    "\n",
    "    es = es.normalize_entity(base_entity_id=\"actions\",\n",
    "                             new_entity_id=\"game_sessions\",\n",
    "                             index=\"game_session\",\n",
    "                             additional_variables=[\"title\", \"type\", \"world\", \n",
    "                                                   \"game_session_y\", \"installation_id\"])\n",
    "\n",
    "    es = es.normalize_entity(base_entity_id=\"game_sessions\",\n",
    "                             new_entity_id=\"game_session_ys\",\n",
    "                             index=\"game_session_y\",\n",
    "                             additional_variables=[\"installation_id\"])\n",
    "\n",
    "    es = es.normalize_entity(base_entity_id=\"game_session_ys\",\n",
    "                             new_entity_id=\"installation_ids\",\n",
    "                             index=\"installation_id\")\n",
    "    \n",
    "    return es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_matrix(dataset_es):\n",
    "    feature_matrix, feature_defs = ft.dfs(entityset=dataset_es, target_entity=\"game_session_ys\")    #1 1/16/2020\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_feature_pipe(dataset, dataset_event_data):\n",
    "    return create_feature_matrix(create_entityset(preprocess_train(dataset, dataset_event_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feature_pipe(dataset, dataset_event_data):\n",
    "    return create_feature_matrix(create_entityset(preprocess_test(dataset, dataset_event_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_dtype = {\n",
    "    'event_id':'object', 'game_session':'object', 'installation_id':'object',\n",
    "    'event_count':'int16', 'event_code':'category', 'game_time':'int32', 'title':'category', \n",
    "    'type':'category', 'world':'category'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "train = dd.read_csv('./input/train.csv', \n",
    "                    parse_dates=['timestamp'], \n",
    "                    dtype=col_dtype,\n",
    "                    usecols=['event_id', 'game_session', 'timestamp', \n",
    "                             'installation_id', 'event_count', 'event_code', \n",
    "                             'game_time', 'title', 'type', 'world'])\n",
    "\n",
    "train_event_data = dd.read_csv('./input/train.csv', \n",
    "                               converters={'event_data': json.loads},\n",
    "                               usecols=['event_data'])\n",
    "\n",
    "# Train target Column\n",
    "train_labels_df = dd.read_csv('./input/train_labels.csv').compute(scheduler='threads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "test = dd.read_csv('./input/test.csv', \n",
    "                    parse_dates=['timestamp'], \n",
    "                    dtype=col_dtype,\n",
    "                    usecols=['event_id', 'game_session', 'timestamp', \n",
    "                             'installation_id', 'event_count', 'event_code', \n",
    "                             'game_time', 'title', 'type', 'world'])\n",
    "\n",
    "test_event_data = dd.read_csv('./input/test.csv', \n",
    "                               converters={'event_data': json.loads},\n",
    "                               usecols=['event_data'])\n",
    "\n",
    "sample_submission = dd.read_csv('./input/sample_submission.csv').compute(scheduler='threads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Features\n",
    "- Create Train Features\n",
    "- train_features = train_feature_pipe(train, train_event_data)\n",
    "- create_feature_matrix(create_entityset(preprocess_train(dataset, dataset_event_data)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating dataframes with unknown divisions.\n",
      "We're assuming that the indexes of each dataframes are \n",
      " aligned. This assumption is not generally safe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## CPU STATS ############\n",
      "3.7.6 (default, Jan  8 2020, 13:42:34) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "32.6\n",
      "svmem(total=17179869184, available=10691715072, percent=37.8, used=5099737088, free=5583634432, active=2628849664, inactive=5091782656, wired=2470887424)\n",
      "memory GB: 3.7191390991210938\n",
      "########## CPU STATS ############\n",
      "Process train_df - done in 294s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Process train_df\"):\n",
    "    # Raw Training Cutting Data\n",
    "    train_df = preprocess_train(train, train_event_data, cutting_sec = 2592000) # Cutting Sec\n",
    "    cpuStats()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## CPU STATS ############\n",
      "3.7.6 (default, Jan  8 2020, 13:42:34) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "26.1\n",
      "svmem(total=17179869184, available=9601208320, percent=44.1, used=5922172928, free=6120763392, active=3436826624, inactive=3470835712, wired=2485346304)\n",
      "memory GB: 3.3749008178710938\n",
      "########## CPU STATS ############\n",
      "Process train_es - done in 72s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Process train_es\"):\n",
    "    # Featuretools EntitySet\n",
    "    train_es = create_entityset(train_df)\n",
    "    cpuStats()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_es.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## CPU STATS ############\n",
      "3.7.6 (default, Jan  8 2020, 13:42:34) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "19.5\n",
      "svmem(total=17179869184, available=11488231424, percent=33.1, used=4109799424, free=8383426560, active=1665593344, inactive=2944565248, wired=2444206080)\n",
      "memory GB: 0.26819610595703125\n",
      "########## CPU STATS ############\n",
      "Featuretools Feature - done in 849s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Featuretools Feature\"):\n",
    "    # Featuretools Feature\n",
    "    train_features = create_feature_matrix(train_es)\n",
    "    cpuStats()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Features add target column 'accuracy_group'\n",
    "\n",
    "train_data_df = (train_features\n",
    "        .merge(train_labels_df[['game_session', 'accuracy_group']], \n",
    "           left_index=True, right_on='game_session', how='inner')\n",
    "        .drop(columns=['game_session', 'installation_id'])\n",
    "        .reset_index(drop=True))\n",
    "\n",
    "# Train Column Schema\n",
    "target_col = 'accuracy_group'\n",
    "feature_cols = train_data_df.columns.drop([target_col]).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons correlation: 0.1344\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "xx = 'MAX(game_sessions.PERCENT_TRUE(actions.correct))'\n",
    "a = train_data_df[xx]\n",
    "b = train_data_df.accuracy_group\n",
    "corr, _ = pearsonr(a, b)\n",
    "print('Pearsons correlation: %.5f' % corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating dataframes with unknown divisions.\n",
      "We're assuming that the indexes of each dataframes are \n",
      " aligned. This assumption is not generally safe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Features - done in 74s\n"
     ]
    }
   ],
   "source": [
    "# Create Test Features\n",
    "#test_features = test_feature_pipe(test, test_event_data)\n",
    "with timer(\"Test Features\"):\n",
    "    # Raw testing Cutting Data\n",
    "    test_df = preprocess_test(test, test_event_data, cutting_sec=259200)\n",
    "\n",
    "    # Featuretools EntitySet\n",
    "    test_es = create_entityset(test_df)\n",
    "\n",
    "    # Featuretools Feature\n",
    "    test_features = create_feature_matrix(test_es)\n",
    "\n",
    "    test_data_df = (test_features\n",
    "        .drop(columns=['installation_id'])\n",
    "        [feature_cols]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-72f0472c0b58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data_df' is not defined"
     ]
    }
   ],
   "source": [
    "test_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
